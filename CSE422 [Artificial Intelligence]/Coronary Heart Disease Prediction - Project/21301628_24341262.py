# -*- coding: utf-8 -*-
"""21301628_24341262.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rtgoAYFr2PEgNc0O7N6O0l4OHrJW5tNi
"""

from google.colab import drive
drive.mount('/content/drive')

"""Importing Required Packeges"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import seaborn as sns

# Packeges for ML Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB


from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import metrics

dataset = pd.read_csv('/content/data_cardiovascular_risk.csv')
dataset.head()

dataset.isnull().sum()

dataset.info()

dataset['sex'].unique()

dataset['is_smoking'].unique()

dataset = dataset.drop(columns='id')
dataset.head()

"""Check For Imbalanced Dataset"""

output_feature = 'TenYearCHD'
class_counts = dataset[output_feature].value_counts()

print("Class distribution:")
print(class_counts)

# Differene in the number of instances per unique classes
if len(class_counts) > 1:
    max_class = class_counts.max()
    min_class = class_counts.min()
    difference_percentage = ((max_class - min_class) / max_class) * 100
    print(f"Percentage difference between the frequency of unique classes: {difference_percentage:.2f}%\n")

plt.figure(figsize=(8, 5))
plt.bar(class_counts.index.astype(str), class_counts.values, color=['skyblue', 'orange'])
plt.title('Class Distribution of Output Feature')
plt.xlabel('Classes')
plt.ylabel('Number of Instances')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

categorical_columns = dataset.select_dtypes(include=['object']).columns.tolist()
encoder = OneHotEncoder(sparse_output=False)

# one-hot encoding to the categorical columns
one_hot_encoded = encoder.fit_transform(dataset[categorical_columns])

#We use get_feature_names_out() to get the column names for the encoded data
one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenate the one-hot encoded dataframe with the original dataframe
df_encoded = pd.concat([dataset, one_hot_df], axis=1)

df_encoded = df_encoded.drop(categorical_columns, axis=1)
# print(df_encoded)
df_encoded.head()

ten_year_chd_column = df_encoded.pop('TenYearCHD')
df_encoded['TenYearCHD'] = ten_year_chd_column
df_encoded.head()

df_encoded.isnull().sum()

columns_with_nulls = df_encoded.columns[df_encoded.isnull().any()].tolist()

for columns in columns_with_nulls:
    df_encoded[columns] = df_encoded[columns].fillna('Unknown')  # Temporarily handle missing
    df_encoded[columns] = df_encoded[columns].astype('category').cat.codes

imputer = IterativeImputer(max_iter=10, random_state=0)
df_encoded[columns_with_nulls] = imputer.fit_transform(df_encoded[columns_with_nulls])
df_encoded.isnull().sum()

imput_it = IterativeImputer()
imput_it.fit_transform(df_encoded)

df_encoded.head() # delete

df_correlation = df_encoded.corr()
df_correlation

# df_correlation.to_csv('correlation_matrix.csv')

sns.heatmap(df_correlation, cmap = 'YlGnBu')

FEATURES = df_encoded.iloc[:, :-1].values
TARGET = df_encoded.iloc[:, -1].values

print(FEATURES[0])
print()
print(TARGET)

"""Logistic Regression"""

x_train, x_test, y_train, y_test = train_test_split(FEATURES, TARGET, test_size=0.3, random_state=42)

logreg = LogisticRegression(solver='liblinear')
logreg.fit(x_train, y_train)

y_pred_test = logreg.predict(x_test)
y_pred_train = logreg.predict(x_train)
train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)

# Print accuracy
print("Training Accuracy:", train_accuracy)
print("Testing Accuracy:", test_accuracy)

conf_matrix = confusion_matrix(y_test, y_pred_test)
print(f"Confusion Matrix:\n{conf_matrix}")

plt.figure(figsize=(8, 5))
plt.bar(['Training Accuracy', 'Testing Accuracy'], [train_accuracy, test_accuracy], color=['skyblue', 'salmon'], alpha=0.8)
plt.ylim(0, 1)
plt.title('Logistic Regression Model Accuracy', fontsize=16)
plt.ylabel('Accuracy', fontsize=14)

plt.text(0, train_accuracy + 0.02, f'{train_accuracy:.2f}', ha='center', fontsize=12)
plt.text(1, test_accuracy + 0.02, f'{test_accuracy:.2f}', ha='center', fontsize=12)
plt.tight_layout()
plt.show()

print("\nClassification Report for Testing Data:")
print(classification_report(y_test, y_pred_test))

report = classification_report(y_test, y_pred_test, output_dict=True)
classes = list(report.keys())[:-3]  # Exclude 'accuracy', 'macro avg', and 'weighted avg'
precision = [report[class_]['precision'] for class_ in classes]
recall = [report[class_]['recall'] for class_ in classes]

x = np.arange(len(classes))
width = 0.35
plt.figure(figsize=(10, 6))
plt.bar(x - width/2, precision, width, label='Precision', color='skyblue')
plt.bar(x + width/2, recall, width, label='Recall', color='salmon')

plt.title('Precision and Recall Comparison (Logistic Regression)', fontsize=16)
plt.xlabel('Classes', fontsize=14)
plt.ylabel('Scores', fontsize=14)
plt.xticks(x, classes, fontsize=12)
plt.ylim(0, 1.3)
plt.legend(fontsize=12)

for i in range(len(classes)):
    plt.text(i - width/2, precision[i] + 0.02, f'{precision[i]:.2f}', ha='center', fontsize=10)
    plt.text(i + width/2, recall[i] + 0.02, f'{recall[i]:.2f}', ha='center', fontsize=10)

plt.tight_layout()
plt.show()

"""KNN"""

KNN = KNeighborsClassifier(n_neighbors=5)
KNN.fit(x_train, y_train)
y_pred = KNN.predict(x_test)
y_pred_For_train = KNN.predict(x_train)

print("Training Accuracy:", metrics.accuracy_score(y_train, y_pred_For_train))
print("Testing Accuracy:", metrics.accuracy_score(y_test, y_pred))

conf_matrix = confusion_matrix(y_test, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

training_accuracy = metrics.accuracy_score(y_train, y_pred_For_train)
testing_accuracy = metrics.accuracy_score(y_test, y_pred)

categories = ['Training Accuracy', 'Testing Accuracy']
accuracies = [training_accuracy, testing_accuracy]

plt.figure(figsize=(8, 5))
plt.bar(['Training Accuracy', 'Testing Accuracy'], [train_accuracy, test_accuracy], color=['skyblue', 'salmon'], alpha=0.8)
plt.ylim(0, 1)
plt.title('KNN Model Accuracy', fontsize=16)
plt.ylabel('Accuracy', fontsize=14)
plt.text(0, train_accuracy + 0.02, f'{training_accuracy:.2f}', ha='center', fontsize=12)
plt.text(1, test_accuracy + 0.02, f'{testing_accuracy:.2f}', ha='center', fontsize=12)
plt.tight_layout()
plt.show()

KNN = KNeighborsClassifier(n_neighbors=5)
KNN.fit(x_train, y_train)
y_pred_test = KNN.predict(x_test)
y_pred_train = KNN.predict(x_train)

print("Classification Report for Testing Data:")
print(classification_report(y_test, y_pred_test))

report = classification_report(y_test, y_pred_test, output_dict=True)
classes = list(report.keys())[:-3]
precision = [report[class_]['precision'] for class_ in classes]
recall = [report[class_]['recall'] for class_ in classes]

x = np.arange(len(classes))
width = 0.35
plt.figure(figsize=(10, 6))
plt.bar(x - width/2, precision, width, label='Precision', color='skyblue')
plt.bar(x + width/2, recall, width, label='Recall', color='salmon')

plt.title('Precision and Recall Comparison (KNN)', fontsize=16)
plt.xlabel('Classes', fontsize=14)
plt.ylabel('Scores', fontsize=14)
plt.xticks(x, classes, fontsize=12)
plt.ylim(0, 1.2)
plt.legend(fontsize=12)

for i in range(len(classes)):
    plt.text(i - width/2, precision[i] + 0.02, f'{precision[i]:.2f}', ha='center', fontsize=10)
    plt.text(i + width/2, recall[i] + 0.02, f'{recall[i]:.2f}', ha='center', fontsize=10)

plt.tight_layout()
plt.show()

"""Naive Bayes"""

model = GaussianNB()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
y_pred_train = model.predict(x_train)

train_accuracy = accuracy_score(y_train, y_pred_train)
print(f"Training Accuracy: {train_accuracy}")
test_accuracy = accuracy_score(y_test, y_pred)
print(f"Testing Accuracy: {test_accuracy}")

conf_matrix = confusion_matrix(y_test, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

categories = ['Training Accuracy', 'Testing Accuracy']
accuracies = [train_accuracy, test_accuracy]

plt.figure(figsize=(6, 5))
plt.bar(categories, accuracies, color=['skyblue', 'salmon'], alpha=0.8)
plt.ylim(0, 1)
plt.title('NB Model Accuracy', fontsize=16)
plt.ylabel('Accuracy', fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)


plt.text(0, train_accuracy + 0.02, f'{train_accuracy:.2f}', ha='center', fontsize=12)
plt.text(1, test_accuracy + 0.02, f'{test_accuracy:.2f}', ha='center', fontsize=12)
plt.tight_layout()
plt.show()

report = classification_report(y_test, y_pred, output_dict=True)
print("Classification Report:")
print(classification_report(y_test, y_pred))

classes = list(report.keys())[:-3]
precision = [report[class_]['precision'] for class_ in classes]
recall = [report[class_]['recall'] for class_ in classes]

x = np.arange(len(classes))
width = 0.35

plt.figure(figsize=(10, 6))
plt.bar(x - width/2, precision, width, label='Precision', color='skyblue')
plt.bar(x + width/2, recall, width, label='Recall', color='salmon')
plt.title('Precision and Recall Comparison (Naive Bayes)', fontsize=16)
plt.xlabel('Classes', fontsize=14)
plt.ylabel('Scores', fontsize=14)
plt.xticks(x, classes, fontsize=12)
plt.ylim(0, 1)
plt.legend(fontsize=12)

for i in range(len(classes)):
    plt.text(i - width/2, precision[i] + 0.02, f'{precision[i]:.2f}', ha='center', fontsize=10)
    plt.text(i + width/2, recall[i] + 0.02, f'{recall[i]:.2f}', ha='center', fontsize=10)

plt.tight_layout()
plt.show()

